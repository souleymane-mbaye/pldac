{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "# Les instructions suivantes sont TRES utile pour recharger automatiquement \n",
    "# le code modifié dans les librairies externes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments, notes = utils.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 lorsque le jeu est jeu  bon  réflexif  joli pour qui est sensible à ce style d illustration    lorsque l on a envie d y rejouer encore pour essayer autrement  et revivre cette histoire de papillons et de voyages    et lorsqu en plus  au détour des règles  on s intéresse également au propos et à la sensibilisation  légère comme les ailes du dit papillons et pourtant aussi bien amené  alors effectivement  il y a là un très bon jeu  ludiquement parlant  et il est possible de s arrêter là  mais également très bon pour l ouverture des yeux des joueuses et joueurs au monde qui nous entoure    ça  c est deux fois bon  \n"
     ]
    }
   ],
   "source": [
    "print(notes[0],comments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = utils.binarisation(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les mots les plus fréquenrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri les mots par ordre décroissant de leurs fréquences, à lancer une seule fois c'est bon\n",
    "mots_freq = utils.words_frequencies(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 50 mots les plus fréquents: ['de', 'et', 'le', 'un', 'est', 'les', 'jeu', 'la', 'des', 'en', 'pas', 'pour', 'on', 'une', 'que', 'qui', 'il', 'mais', 'ce', 'du', 'plus', 'je', 'dans', 'ne', 'avec', 'très', 'bien', 'au', 'qu', 'tout', 'se', 'sur', 'ou', 'sont', 'vous', 'cartes', 'peu', 'par', 'même', 'si', 'joueurs', 'partie', 'fait', 'bon', 'peut', 'ai', 'faire', 'parties', 'ça', 'jouer']\n"
     ]
    }
   ],
   "source": [
    "k = 50\n",
    "mots_k =[m for m,f in mots_freq[:k]]\n",
    "print(f'Les {k} mots les plus fréquents: {mots_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variantes et évaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Logistic Regression\n",
      "Logistic regression accuracy: 0.8333029104553291  f1-score: 0.8461338763737748\n",
      "Top 10 mots positifs:  ['excellent', 'bémol', 'axé', 'foncez', 'picto', 'satisfait', 'rythmées', 'hilarant', 'abuser', 'automatiquement']\n",
      "Top 10 mots négatifs:  ['déception', 'arnaque', 'error', 'ennuyeux', 'morpion', 'décevant', 'échappé', 'gâché', 'indigeste', 'once']\n"
     ]
    }
   ],
   "source": [
    "model, vectorizer = utils.count_vectorizer(comments, notes)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: SVM\n",
      "Svm accuracy: 0.8361220971503904  f1-score: 0.8483695276869055\n",
      "Top 10 mots positifs:  ['excellent', 'bémol', 'bon', 'bravo', 'hésitez', 'foncez', 'excellente', 'evidemment', 'reproche', 'efficace']\n",
      "Top 10 mots négatifs:  ['déception', 'décevant', 'ennuyeux', 'arnaque', 'intérêt', 'aucune', 'ennui', 'préférez', 'malheureusement', 'ennuyé']\n"
     ]
    }
   ],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Logistic Regression\n",
      "Logistic regression accuracy: 0.8309096440523274  f1-score: 0.8442276241174973\n",
      "Top 10 mots positifs:  ['excellent', 'bémol', 'foncez', 'axé', 'picto', 'automatiquement', 'parfaite', 'pépite', 'tempête', 'atypique']\n",
      "Top 10 mots négatifs:  ['déception', 'arnaque', 'error', 'ennuyeux', 'décevant', 'crossing', 'morpion', 'ennuyé', 'revendu', 'échappé']\n"
     ]
    }
   ],
   "source": [
    "model, vectorizer = utils.count_vectorizer(comments, notes, stop_words=stopwords.words('french'))\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: SVM\n",
      "Svm accuracy: 0.8336679849913802  f1-score: 0.8463901127658807\n",
      "Top 10 mots positifs:  ['excellent', 'bémol', 'bravo', 'evidemment', 'bon', 'parfaite', 'efficace', 'atypique', 'excellente', 'axé']\n",
      "Top 10 mots négatifs:  ['déception', 'arnaque', 'ennuyeux', 'décevant', 'intérêt', 'ennui', 'ennuyé', 'aucune', 'préférez', 'error']\n"
     ]
    }
   ],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, stop_words=stopwords.words('french'))\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopswords et les 200 termes les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Logistic Regression\n",
      "Logistic regression accuracy: 0.8190447216306662  f1-score: 0.8344140588884539\n",
      "Top 10 mots positifs:  ['bémol', 'jouissif', 'foncez', 'addictif', 'bémols', 'tempête', 'automatiquement', 'biblios', 'redemande', 'surprend']\n",
      "Top 10 mots négatifs:  ['déception', 'error', 'arnaque', 'ennuyeux', 'décevant', 'ennuyé', 'revendu', 'indigeste', 'poussière', 'morpion']\n"
     ]
    }
   ],
   "source": [
    "k = 200\n",
    "mots_k =[m for m,f in mots_freq[:k]]\n",
    "stop_words = mots_k + stopwords.words('french')\n",
    "model, vectorizer = utils.count_vectorizer(comments, notes, stop_words=stop_words)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Logistic Regression\n",
      "Logistic regression accuracy: 0.8233647703072711  f1-score: 0.8372602084017874\n",
      "Top 10 mots positifs:  ['bémol', 'efficace', 'parfait', 'must', 'adore', 'fluide', 'excellente', 'bravo', 'bonheur', 'accessible']\n",
      "Top 10 mots négatifs:  ['déception', 'intérêt', 'ennuyeux', 'aucune', 'ennuie', 'ennui', 'bof', 'malheureusement', 'ennuyé', 'décevant']\n"
     ]
    }
   ],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, stop_words=stop_words)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopswords et les 2 000 termes les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Random Forest\n",
      "Random forest accuracy: 0.8281918669506135  f1-score: 0.80402695274928\n",
      "Poids des mots positifs et négatifs non connus, entre guillemets !\n"
     ]
    }
   ],
   "source": [
    "k = 2_000\n",
    "mots_k =[m for m,f in mots_freq[:k]]\n",
    "stop_words = mots_k + stopwords.words('french')\n",
    "model, vectorizer = utils.count_vectorizer(comments, notes, stop_words=stop_words)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtfidf_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m utils\u001b[38;5;241m.\u001b[39mtop_words(model, vectorizer)\n",
      "File \u001b[1;32m~\\Desktop\\PLDAC\\reco2019-master\\reco2019-master\\utils.py:187\u001b[0m, in \u001b[0;36mtfidf_vectorizer\u001b[1;34m(comments, classes, nbins, **tfidf_vectorizer_args)\u001b[0m\n\u001b[0;32m    184\u001b[0m new_classes \u001b[38;5;241m=\u001b[39m binarisation(classes,nb\u001b[38;5;241m=\u001b[39mnbins)\n\u001b[0;32m    185\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtfidf_vectorizer_args)\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclassifieur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\PLDAC\\reco2019-master\\reco2019-master\\utils.py:122\u001b[0m, in \u001b[0;36mclassifieur\u001b[1;34m(vectorizer, comments, classes)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassifieur\u001b[39m(vectorizer,comments,classes):\n\u001b[1;32m--> 122\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(X, classes, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    124\u001b[0m     rus \u001b[38;5;241m=\u001b[39m RandomUnderSampler(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1273\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\Desktop\\PLDAC\\reco2019-master\\reco2019-master\\utils.py:54\u001b[0m, in \u001b[0;36mstem\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     50\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m FrenchStemmer()\n\u001b[0;32m     52\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(doc)\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m---> 54\u001b[0m stemmed_tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     55\u001b[0m stemmed_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmed_tokens)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stemmed_text\n",
      "File \u001b[1;32m~\\Desktop\\PLDAC\\reco2019-master\\reco2019-master\\utils.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m FrenchStemmer()\n\u001b[0;32m     52\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(doc)\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m---> 54\u001b[0m stemmed_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     55\u001b[0m stemmed_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmed_tokens)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stemmed_text\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\stem\\snowball.py:2555\u001b[0m, in \u001b[0;36mFrenchStemmer.stem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   2552\u001b[0m     word \u001b[38;5;241m=\u001b[39m word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   2554\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step4_suffixes:\n\u001b[1;32m-> 2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   2556\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m rv:\n\u001b[0;32m   2557\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m r2 \u001b[38;5;129;01mand\u001b[39;00m rv[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, stop_words=stop_words, preprocessor=utils.stem)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopswords et les 20 000 termes les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m mots_k \u001b[38;5;241m=\u001b[39m[m \u001b[38;5;28;01mfor\u001b[39;00m m,f \u001b[38;5;129;01min\u001b[39;00m mots_freq[:k]]\n\u001b[0;32m      3\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m mots_k \u001b[38;5;241m+\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m utils\u001b[38;5;241m.\u001b[39mtop_words(model, vectorizer)\n",
      "File \u001b[1;32m~\\Desktop\\PLDAC\\reco2019-master\\reco2019-master\\utils.py:180\u001b[0m, in \u001b[0;36mcount_vectorizer\u001b[1;34m(comments, classes, nbins, **count_vectorizer_args)\u001b[0m\n\u001b[0;32m    177\u001b[0m new_classes \u001b[38;5;241m=\u001b[39m binarisation(classes,nb\u001b[38;5;241m=\u001b[39mnbins)\n\u001b[0;32m    178\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcount_vectorizer_args)\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclassifieur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\PLDAC\\reco2019-master\\reco2019-master\\utils.py:122\u001b[0m, in \u001b[0;36mclassifieur\u001b[1;34m(vectorizer, comments, classes)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassifieur\u001b[39m(vectorizer,comments,classes):\n\u001b[1;32m--> 122\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(X, classes, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    124\u001b[0m     rus \u001b[38;5;241m=\u001b[39m RandomUnderSampler(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1273\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:116\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:249\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[0;32m    252\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m]\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[0;32m    252\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 20_000\n",
    "mots_k =[m for m,f in mots_freq[:k]]\n",
    "stop_words = mots_k + stopwords.words('french')\n",
    "model, vectorizer = utils.count_vectorizer(comments, notes, stop_words=stop_words)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, stop_words=stop_words)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.count_vectorizer(comments, notes, stop_words=stop_words,preprocessor=utils.stem)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes,  ngram_range=(1, 2))\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.count_vectorizer(comments, notes, ngram_range=(1, 2), max_df=0.5, stop_words=stopwords.words('french'))\n",
    "utils.top_words(model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, ngram_range=(1, 2), max_df=0.5, stop_words=stopwords.words('french'))\n",
    "utils.top_words(model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, stop_words=stop_words)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.count_vectorizer(comments, notes, ngram_range=(1, 3))\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.count_vectorizer(comments, notes, ngram_range=(1, 3))\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.count_vectorizer(comments, notes, ngram_range=(1, 3), max_df=0.5, stop_words=stopwords.words('french'))\n",
    "utils.top_words(model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, ngram_range=(1, 2), max_df=0.5, stop_words=stopwords.words('french'))\n",
    "utils.top_words(model, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('french')\n",
    "model, vectorizer = utils.count_vectorizer(comments, notes, stop_words=stop_words,preprocessor=utils.stem)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, stop_words=stop_words,preprocessor=utils.stem)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('french')\n",
    "model, vectorizer = utils.count_vectorizer(comments, notes, stop_words=stop_words,preprocessor=utils.lemmatize)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = utils.tfidf_vectorizer(comments, notes, stop_words=stop_words,preprocessor=utils.lemmatize)\n",
    "utils.top_words(model, vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
